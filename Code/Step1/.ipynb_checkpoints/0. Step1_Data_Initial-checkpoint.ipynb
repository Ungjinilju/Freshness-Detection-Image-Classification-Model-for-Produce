{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd880851",
   "metadata": {},
   "source": [
    "# Guide\n",
    "실행 불필요  \n",
    "전처리 완료된 데이터 링크 아래 참고\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e40ebc1",
   "metadata": {},
   "source": [
    "# Data\n",
    "[[Google Drive] /Data/Initial/Step1.zip](https://drive.google.com/file/d/1LpnhjeBOoLH-QwbVOovCk-OpdpmIQAaZ/view?usp=sharing)\n",
    "\n",
    "## Dataset Source\n",
    "[[Kaggle] Fruit and Vegetable Disease (Healthy vs Rotten)](https://www.kaggle.com/datasets/muhammad0subhan/fruit-and-vegetable-disease-healthy-vs-rotten)  \n",
    "apple 데이터만 사용\n",
    "\n",
    "## Preprocessing\n",
    "기존 데이터셋에는 이미 증강 이미지 존재(일부 클래스에만) → 다른 Step의 모델에도 사용하기 위해 증강 이미지 삭제 + 중복 이미지 삭제  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078ce688",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "`ImageDataGenerator` 사용\n",
    "\n",
    "## Augmentation Ratio\n",
    "1개의 이미지 당 2개의 증강 이미지 생성\n",
    "\n",
    "## Data\n",
    "|              |&nbsp;&nbsp;&nbsp;Original&nbsp;&nbsp;&nbsp;| Augmentation |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Final&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|\n",
    "|:------------:|:------------:|:------------:|:------------:|\n",
    "| Apple_Fresh  | 368 | 728 | 1096 |\n",
    "| Apple_Rotten | 462 | 917 | 1379 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5f3d305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 13:54:34.898288: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d6725ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=90,            # 회전 각도 범위 [-90, +90]\n",
    "    shear_range=0.3,              # 전단 변형(층밀림) 각도 [0, 0.3](radian):최대 약 17도\n",
    "    zoom_range=[0.7, 1.3],        # 확대/축소 범위. 원본의 70% ~ 130%\n",
    "    brightness_range=[0.2, 1.3],  # 밝기 조정 범위. 원본의 20% ~ 130%\n",
    "    horizontal_flip=True,         # 좌우 반전\n",
    "    vertical_flip=True,           # 상하 반전\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1a4a171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(input_dir, output_dir):\n",
    "    \n",
    "    for filename in os.listdir(input_dir):\n",
    "        \n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            \n",
    "            img_path = os.path.join(input_dir, filename)\n",
    "            img = load_img(img_path)  # 이미지 로드\n",
    "            x = img_to_array(img)     # numpy 배열로 변환\n",
    "            x = np.expand_dims(x, axis=0)  # 배치 차원 추가\n",
    "            \n",
    "            # 원본 이미지 저장\n",
    "            original_output_path = os.path.join(output_dir, f'original_{filename}')\n",
    "            img.save(original_output_path)  # 원본 이미지 저장\n",
    "        \n",
    "            # 증강 이미지 생성 및 저장\n",
    "            i = 0\n",
    "            for batch in datagen.flow(x, batch_size=1, save_to_dir=output_dir, save_prefix=f'aug_{filename.split(\".\")[0]}', save_format='jpg'):\n",
    "                i += 1\n",
    "                if i >= 2:  # 하나의 이미지당 2개의 증강 이미지\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d2e3f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/tf/Fixed_Data/Data_Initial\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7801193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apple_Fresh\n",
    "input_dir = os.path.join(base_dir, 'apple/fresh')\n",
    "output_dir = os.path.join(base_dir, 'Augmented_Data/Apple_Fresh')\n",
    "\n",
    "data_augmentation(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3da37234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apple_Rotten\n",
    "input_dir = os.path.join(base_dir, 'apple/stale')\n",
    "output_dir = os.path.join(base_dir, 'Augmented_Data/Apple_Rotten')\n",
    "\n",
    "data_augmentation(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a198768a",
   "metadata": {},
   "source": [
    "# Data Split\n",
    "## Original & Augmented Data\n",
    "/tf/Fixed_Data/Data_Initial/Augmented_Data  \n",
    "│  \n",
    "├── Apple_Fresh  \n",
    "│  \n",
    "└── Apple_Rotten  \n",
    "\n",
    "## Splited Data\n",
    "/tf/Fixed_Data/Data_Initial/Step1  \n",
    "│  \n",
    "├── train  \n",
    "│&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├── Apple_Fresh  \n",
    "│&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└── Apple_Rotten  \n",
    "│  \n",
    "├── validation  \n",
    "│&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├── Apple_Fresh  \n",
    "│&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└── Apple_Rotten  \n",
    "│  \n",
    "└── test  \n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├── Apple_Fresh  \n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└── Apple_Rotten  \n",
    "\n",
    "    \n",
    "## Split Ratio\n",
    "train : validation : test = 7 : 2 : 1\n",
    "\n",
    "## Dataset\n",
    "|              | &nbsp;&nbsp;&nbsp; train &nbsp;&nbsp;&nbsp; | validation | &nbsp;&nbsp;&nbsp;&nbsp; test &nbsp;&nbsp;&nbsp;&nbsp; |\n",
    "|:------------:|:------------:|:------------:|:------------:|\n",
    "| Apple_Fresh  | 767 | 219 | 110 |\n",
    "| Apple_Rotten | 965 | 275 | 139 |\n",
    "| Total | 1732 | 494 | 249 |\n",
    "\n",
    "## Code\n",
    "교재 Chapter 8-2 서브셋 저장 코드 참고\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e48c364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ad9e54",
   "metadata": {},
   "source": [
    "### Source : 교재 코드 8-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c5f0315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 리스트를 서브셋으로 저장\n",
    "def make_subset(subset_name, file_list):\n",
    "    \n",
    "    for file_path, category in file_list:\n",
    "        \n",
    "        dst_dir = os.path.join(new_base_dir, subset_name, category)\n",
    "        os.makedirs(dst_dir, exist_ok=True)\n",
    "        shutil.copy(file_path, os.path.join(dst_dir, os.path.basename(file_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "945339f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 비율로 데이터 분할 후 서브셋 생성\n",
    "def split_data(base_dir, train_ratio, val_ratio):\n",
    "    \n",
    "    # 클래스 디렉토리 수집\n",
    "    class_dirs = [d for d in os.listdir(base_dir) \n",
    "                  if os.path.isdir(os.path.join(base_dir, d)) and not d.startswith('.')]\n",
    "    \n",
    "    train_data = 0\n",
    "    val_data = 0\n",
    "    test_data = 0\n",
    "    \n",
    "    # 데이터 분할\n",
    "    for class_dir in class_dirs:\n",
    "        \n",
    "        class_path = os.path.join(base_dir, class_dir)\n",
    "        \n",
    "        # 파일 목록 수집\n",
    "        files = [(os.path.join(class_path, f), class_dir) \n",
    "                 for f in os.listdir(class_path) if not f.startswith('.')]\n",
    "        \n",
    "        # 데이터 shuffle\n",
    "        random.shuffle(files)\n",
    "\n",
    "        # 데이터 분할\n",
    "        total = len(files)\n",
    "        train_index = int(total * train_ratio)\n",
    "        val_index = train_index + int(total * val_ratio)\n",
    "\n",
    "        train_files = files[:train_index]\n",
    "        val_files = files[train_index:val_index]\n",
    "        test_files = files[val_index:]\n",
    "\n",
    "        # 서브셋 생성\n",
    "        make_subset(\"train\", train_files)\n",
    "        make_subset(\"validation\", val_files)\n",
    "        make_subset(\"test\", test_files)\n",
    "        \n",
    "        train_data += len(train_files)\n",
    "        val_data += len(val_files)\n",
    "        test_data += len(test_files)\n",
    "\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4575e917",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dir = \"/tf/Fixed_Data/Data_Initial/Augmented_Data\"\n",
    "new_base_dir = \"/tf/Fixed_Data/Data_Initial/Step1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5e6c52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train:validation:test = 7:2:1\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b0e9912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train files: 1732, Validation files: 494, Test files: 249\n"
     ]
    }
   ],
   "source": [
    "# 데이터 분할 및 서브셋 생성\n",
    "train_count, val_count, test_count = split_data(original_dir, train_ratio, val_ratio)\n",
    "\n",
    "print(f\"Train files: {train_count}, Validation files: {val_count}, Test files: {test_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
